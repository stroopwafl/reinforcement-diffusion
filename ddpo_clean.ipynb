{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fea26e-0a33-435a-ac8b-9ffbbd116873",
   "metadata": {},
   "source": [
    "# DDPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11b689-6615-482d-9c44-ec042e2c35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math, random, torch, matplotlib.pyplot as plt, numpy as np, matplotlib as mpl, shutil, os, gzip, pickle, re, copy\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from itertools import zip_longest\n",
    "from functools import partial\n",
    "import fastcore.all as fc\n",
    "from glob import glob\n",
    "\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import lr_scheduler\n",
    "from diffusers import UNet2DModel\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torch.nn import init\n",
    "\n",
    "from miniai.learner import *\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.activations import *\n",
    "from miniai.core import *\n",
    "from miniai.initialisation import *\n",
    "from miniai.accel import *\n",
    "from miniai.ddpm import *\n",
    "from miniai.resnet import *\n",
    "from miniai.aug import *\n",
    "from miniai.fid import *\n",
    "from miniai.diffusion import *\n",
    "\n",
    "from einops import rearrange\n",
    "from fastprogress import progress_bar\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b851785-4dea-420b-be2a-e40a4205115f",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18770674-bbff-4ce8-aa74-8bf800f07b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(animal, activity): \n",
    "    prefix = \"an\" if animal[0] in ('a','e','i','o','u') else \"a\"\n",
    "    return f\"{prefix} {animal} {activity}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfb4b7-5332-4bed-bf23-b6bfd9501172",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/projects/reinforcement_finetuning'\n",
    "animals = open(f'{path}/animals.txt', mode='r').readlines()\n",
    "activities = open(f'{path}/activities.txt', mode='r').readlines()\n",
    "\n",
    "animals = [i.replace('\\n', '') for i in animals]\n",
    "activities = [i.replace('\\n', '') for i in activities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467ebcc-cfb3-4568-b5d2-f697f2a68d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "str2id = {}\n",
    "count = 0\n",
    "for i in animals:\n",
    "    for j in activities:\n",
    "        str2id[get_prompt(i, j)] = count\n",
    "        count += 1\n",
    "        \n",
    "id2str = {v:k for k,v in zip(str2id.keys(),str2id.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755508ec-ea06-43ff-8fc8-78dcb1cdb8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDS:\n",
    "    def __init__(self, animals:list, activities:list, length:int): \n",
    "        self.animals = [animals[torch.randint(len(animals), (1,))] for i in range(length)]\n",
    "        self.activities = [activities[torch.randint(len(activities), (1,))] for i in range(length)]\n",
    "        self.seeds = [int(random.random()*1000000) for _ in range(length)]\n",
    "    def __len__(self): return len(self.animals)\n",
    "    def __getitem__(self, i): \n",
    "        return (str2id[get_prompt(self.animals[i], self.activities[i])], self.seeds[i]), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e164b0c-01ab-496c-9355-a7160f22dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self, train, valid, bs, **kwargs): \n",
    "        fc.store_attr()\n",
    "        self._make_dataloaders()\n",
    "    def _make_dataloaders(self):\n",
    "        self.train = DataLoader(self.train, batch_size=self.bs, num_workers=4, collate_fn=default_collate, shuffle=True)\n",
    "        self.valid = DataLoader(self.valid, batch_size=self.bs, num_workers=4, collate_fn=default_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b143488-ce6b-410a-a83d-3741c01dd5e1",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a5016-e501-4229-bf06-21106cd330a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='text_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf3bc3-b204-436d-9e9a-45f8604ea8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(prompt, unconditional=False, device='cpu'):\n",
    "    text_input = tokeniser(prompt, padding=\"max_length\", max_length=tokeniser.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokeniser([\"\"], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeds = text_encoder(text_input.input_ids)[0]\n",
    "        if unconditional:\n",
    "            uncond_embeddings = text_encoder(uncond_input.input_ids)[0]\n",
    "            embeds = torch.cat([uncond_embeddings, embeds])\n",
    "    return embeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0d851-8f59-4f1b-8ebe-203855c7262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2embed = {k:get_embeddings(v).cpu() for k,v in id2str.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eddfc9-9802-455b-b19b-2cd26c277013",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2embed[135] = get_embeddings(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d32fb-3d69-4076-b104-1f6307bc0982",
   "metadata": {},
   "source": [
    "### Prompt alignment objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a631000-1f17-4339-a66c-a317d291cf5f",
   "metadata": {},
   "source": [
    "#### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80800c2e-21ab-4683-889b-b7e7db8e4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "bert = BERTScorer(model_type='roberta-large', lang='en', rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a282fe-917b-4bde-9bab-fd05e965f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Here is my shitty text for testing Bert']\n",
    "compare_1 = [\"Butt butt boop boop flip flobble\"]\n",
    "compare_2 = [\"Here is my delightfully well crafted text for testing Bert\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4eca09-ca1e-456d-91d8-9bd20fff56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1 = bert.score(text, compare_1)\n",
    "p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6e7f3-7d86-4e49-a066-9c966fa3af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1 = bert.score(text, compare_2)\n",
    "p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c455a-0cb1-450e-8c1d-1aca11e970c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1 = bert.score(text, text)\n",
    "p, r, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635b776-37df-4339-94da-45553bcca9b9",
   "metadata": {},
   "source": [
    "#### LLaVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe03fb-21f0-4108-8ac5-df476f42ba3b",
   "metadata": {},
   "source": [
    "##### Hide for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b3617-06e3-41a6-93ee-9c0808b56c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /home/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n",
    "    --input_dir /home/models/foundation/LLaMA/ --model_size 7B --output_dir /home/models/foundation/LLaMA/7b/llama_7b_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41e15d-8622-4191-b536-10b55883b912",
   "metadata": {},
   "source": [
    "Apply the delta function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c1511-6e1d-4b33-99d0-06803bbc4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python /home/models/foundation/LLaVA/llava/model/apply_delta.py \\\n",
    "#     --base /home/models/foundation/LLaMA/7B/llama_7b_hf \\\n",
    "#     --target /home/models/foundation/LLaVA/7B/llava_7b_hf \\\n",
    "#     --delta liuhaotian/LLaVA-7b-delta-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654939e-7369-4af1-9d4d-8605065fef61",
   "metadata": {},
   "source": [
    "##### Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfeb2e8-cf73-4409-9dfa-9e0d037c7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\n",
    "from llava.model import *\n",
    "from llava.model.utils import KeywordsStoppingCriteria\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218e171-8b0c-43dc-ac3c-55ef5369258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ba282-be8b-441f-8c7e-1bb7b48e933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(tokenizer):\n",
    "    \"\"\"Adds new tokens to the tokenizer.\"\"\"\n",
    "    tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4b928-6028-4be2-ba16-9fc1974de24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/models/foundation/LLaVA/7B/llava-7B-v0/'\n",
    "llava_tokenizer = setup_tokenizer(AutoTokenizer.from_pretrained(path))\n",
    "llava_model = LlavaLlamaForCausalLM.from_pretrained(path, low_cpu_mem_usage=True, torch_dtype=torch.float16, use_cache=True).cuda()\n",
    "image_processor = CLIPImageProcessor.from_pretrained(llava_model.config.mm_vision_tower, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2e10b-c98b-4336-b325-be22f8d3c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vision_tower(model, tokenizer, mm_use_im_start_end=True):\n",
    "    vision_tower = model.get_model().vision_tower[0]\n",
    "    if vision_tower.device.type == 'meta':\n",
    "        vision_tower = CLIPVisionModel.from_pretrained(vision_tower.config._name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True).cuda()\n",
    "        model.get_model().vision_tower[0] = vision_tower\n",
    "    else:\n",
    "        vision_tower.to(device='cuda', dtype=torch.float16)\n",
    "    vision_config = vision_tower.config\n",
    "    vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n",
    "    vision_config.use_im_start_end = mm_use_im_start_end\n",
    "    if mm_use_im_start_end:\n",
    "        vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n",
    "    return model, vision_config, vision_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30226d4b-cfb0-49e7-ba5d-ee67ad986fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llava_prompt(query, image_token_len, mm_use_im_start_end=True):\n",
    "    if mm_use_im_start_end:\n",
    "        query = query + '\\n' + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN\n",
    "    else:\n",
    "        query = query + '\\n' + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len\n",
    "    \n",
    "    conv_mode = \"multimodal\"\n",
    "\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], query)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv, conv.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef54041-d53b-4da7-b584-c5f26025f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_query(query, image, model, clip, tokenizer, device='cuda'):\n",
    "    # Assumes that the tokenizer has been setup with setup_tokenizer\n",
    "    mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "    model, vision_config, vision_tower = setup_vision_tower(model, tokenizer, mm_use_im_start_end)\n",
    "    \n",
    "    image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\n",
    "    conv, prompt = get_llava_prompt(query, image_token_len)\n",
    "    input_ids = torch.as_tensor(tokenizer([prompt]).input_ids).to(device)\n",
    "    image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0].to(device)\n",
    "\n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image.unsqueeze(0).half(),\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=64,\n",
    "            stopping_criteria=[stopping_criteria])\n",
    "\n",
    "    input_token_len = input_ids.shape[1]\n",
    "    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n",
    "    if n_diff_input_output > 0:\n",
    "        print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\n",
    "    outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n",
    "    outputs = outputs.strip()\n",
    "    if outputs.endswith(stop_str):\n",
    "        outputs = outputs[:-len(stop_str)]\n",
    "    return outputs.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589d5a6-749f-4347-af6b-7073334300a7",
   "metadata": {},
   "source": [
    "Finally, we can wrap this functionality into a top-level function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52682c5-b54d-4bf9-80ef-df7c1aca3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_alignment_score(prompt, image, query=\"\", llava_model=None, clip_model=None, llava_tokenizer=None, bert=None, return_all=False):\n",
    "    image = (image.clip(-1,1) + 1) / 2\n",
    "    llava_out = llava_query(query, image, llava_model, clip_model, llava_tokenizer)\n",
    "    p, r, f1 = bert.score([prompt], [llava_out])\n",
    "    return (p,r,f1) if return_all else r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05069e40-8945-4e62-8222-15bd865042ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(p, model, seed=None, height=512, width=512, steps=50, guidance_scale=5, device='cuda'):\n",
    "    if seed is None: seed = int(torch.rand((1,)) * 1000000)\n",
    "    embeddings = torch.cat([id2embed[135], id2embed[p.item()]]).to(device)\n",
    "    scheduler.set_timesteps(steps)\n",
    "    shape = (1, model.in_channels, height // 8, width // 8)\n",
    "    latents = torch.randn(shape, generator=torch.manual_seed(seed)).to(device)\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    \n",
    "    for i, t in enumerate(progress_bar(scheduler.timesteps, leave=False)):\n",
    "        latent_model_input = torch.cat([latents] * 2).to(device)\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = model(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad(): image = vae.decode(latents).sample\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c33125-c55b-475a-b14f-59016135efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, AutoencoderKL, LMSDiscreteScheduler, DDIMScheduler\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='vae').to('cuda')\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule='scaled_linear')\n",
    "sd_model = UNet2DConditionModel.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='unet').to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5686c0-b3c1-4666-af9a-fb811a6aae0a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27233717-ed60-461f-9970-6b5e021c27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(samples, model, scheduler, height=512, width=512, steps=50, guidance_scale=5, device='cuda'):\n",
    "    model.eval()\n",
    "    for i, sample in progress_bar(samples.items(), leave=False, comment='generating samples...'):\n",
    "        seed = sample['seed'] if sample['seed'] is not None else int(torch.rand((1,)) * 1000000)\n",
    "        embeddings = torch.cat([id2embed[135], id2embed[sample['prompt'].item()]]).to(device)\n",
    "        scheduler.set_timesteps(steps)\n",
    "        shape = (1, model.in_channels, height // 8, width // 8)\n",
    "        latents = torch.randn(shape, generator=torch.manual_seed(seed)).to(device)\n",
    "        latents = latents * scheduler.init_noise_sigma\n",
    "        \n",
    "        for i, t in enumerate(progress_bar(scheduler.timesteps, leave=False)):\n",
    "            sample['latents'].append(latents)\n",
    "            latent_model_input = torch.cat([latents] * 2).to(device)\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = model(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            latents, log_prob = scheduler.step(noise_pred, t, latents)\n",
    "            sample['next_latents'].append(latents)\n",
    "            sample['log_prob'].append(log_prob)\n",
    "\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        with torch.no_grad(): image = vae.decode(latents).sample\n",
    "        sample['final_image'] = image\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223a494-7b5a-4cc1-9990-8d222a4c43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningStats:\n",
    "    def __init__(self, min_count=2, eps=1e-6):\n",
    "        fc.store_attr()\n",
    "        self.stats = {}\n",
    "    def get_norm_rewards(self, prompts, rewards):\n",
    "        norm_rewards = []\n",
    "        self._update(prompts.cpu(), rewards.cpu())\n",
    "        for i, p in enumerate(prompts):\n",
    "            p = p.item()\n",
    "            if len(self.stats[p]) >= self.min_count:\n",
    "                mean = torch.cat(self.stats[p]).mean()\n",
    "                std = torch.cat(self.stats[p]).std() + self.eps\n",
    "                norm_rewards.append((rewards[i] - mean) / std)\n",
    "            else:\n",
    "                norm_rewards.append(rewards[i])\n",
    "        return torch.stack(norm_rewards)\n",
    "    def _update(self, prompts, rewards):\n",
    "        for p in torch.unique(prompts):\n",
    "            p = p.item()\n",
    "            if not p in self.stats: self.stats[p] = []\n",
    "            p_rewards = rewards[prompts == p]\n",
    "            for r in p_rewards: self.stats[p].append(r.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29c691-13fd-4d15-a995-bc6ffc222e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler\n",
    "from diffusers.utils import randn_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec9802-9d44-4f69-b29d-71e44902ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIMScheduler(DDIMScheduler):\n",
    "    def step(\n",
    "        self,\n",
    "        model_output: torch.FloatTensor,\n",
    "        timestep: int,\n",
    "        sample: torch.FloatTensor,\n",
    "        eta: float = 1.0,\n",
    "        use_clipped_model_output: bool = False,\n",
    "        generator=None,\n",
    "        variance_noise: Optional[torch.FloatTensor] = None,\n",
    "        return_dict: bool = True,\n",
    "        prev_sample: Optional[torch.FloatTensor] = None\n",
    "    ) -> Union[DDIMSchedulerOutput, Tuple]:\n",
    "        \"\"\"\n",
    "        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n",
    "        process from the learned model outputs (most often the predicted noise).\n",
    "        Args:\n",
    "            model_output (`torch.FloatTensor`): direct output from learned diffusion model.\n",
    "            timestep (`int`): current discrete timestep in the diffusion chain.\n",
    "            sample (`torch.FloatTensor`):\n",
    "                current instance of sample being created by diffusion process.\n",
    "            eta (`float`): weight of noise for added noise in diffusion step.\n",
    "            use_clipped_model_output (`bool`): if `True`, compute \"corrected\" `model_output` from the clipped\n",
    "                predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when\n",
    "                `self.config.clip_sample` is `True`. If no clipping has happened, \"corrected\" `model_output` would\n",
    "                coincide with the one provided as input and `use_clipped_model_output` will have not effect.\n",
    "            generator: random number generator.\n",
    "            variance_noise (`torch.FloatTensor`): instead of generating noise for the variance using `generator`, we\n",
    "                can directly provide the noise for the variance itself. This is useful for methods such as\n",
    "                CycleDiffusion. (https://arxiv.org/abs/2210.05559)\n",
    "            return_dict (`bool`): option for returning tuple rather than DDIMSchedulerOutput class\n",
    "        Returns:\n",
    "            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] or `tuple`:\n",
    "            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When\n",
    "            returning a tuple, the first element is the sample tensor.\n",
    "        \"\"\"\n",
    "        if self.num_inference_steps is None:\n",
    "            raise ValueError(\n",
    "                \"Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler\"\n",
    "            )\n",
    "\n",
    "        # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf\n",
    "        # Ideally, read DDIM paper in-detail understanding\n",
    "\n",
    "        # Notation (<variable name> -> <name in paper>\n",
    "        # - pred_noise_t -> e_theta(x_t, t)\n",
    "        # - pred_original_sample -> f_theta(x_t, t) or x_0\n",
    "        # - std_dev_t -> sigma_t\n",
    "        # - eta -> η\n",
    "        # - pred_sample_direction -> \"direction pointing to x_t\"\n",
    "        # - pred_prev_sample -> \"x_t-1\"\n",
    "\n",
    "        # 1. get previous step value (=t-1)\n",
    "        prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps\n",
    "\n",
    "        # 2. compute alphas, betas\n",
    "        alpha_prod_t = self.alphas_cumprod[timestep.cpu()]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_timestep.cpu()] if prev_timestep >= 0 else self.final_alpha_cumprod\n",
    "\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "        # 3. compute predicted original sample from predicted noise also called\n",
    "        # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        if self.config.prediction_type == \"epsilon\":\n",
    "            pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "            pred_epsilon = model_output\n",
    "        elif self.config.prediction_type == \"sample\":\n",
    "            pred_original_sample = model_output\n",
    "            pred_epsilon = (sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)\n",
    "        elif self.config.prediction_type == \"v_prediction\":\n",
    "            pred_original_sample = (alpha_prod_t**0.5) * sample - (beta_prod_t**0.5) * model_output\n",
    "            pred_epsilon = (alpha_prod_t**0.5) * model_output + (beta_prod_t**0.5) * sample\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or\"\n",
    "                \" `v_prediction`\"\n",
    "            )\n",
    "\n",
    "        # 4. Clip or threshold \"predicted x_0\"\n",
    "        if self.config.thresholding:\n",
    "            pred_original_sample = self._threshold_sample(pred_original_sample)\n",
    "        elif self.config.clip_sample:\n",
    "            pred_original_sample = pred_original_sample.clamp(\n",
    "                -self.config.clip_sample_range, self.config.clip_sample_range\n",
    "            )\n",
    "\n",
    "        # 5. compute variance: \"sigma_t(η)\" -> see formula (16)\n",
    "        # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n",
    "        variance = self._get_variance(timestep, prev_timestep)\n",
    "        std_dev_t = eta * variance ** (0.5)\n",
    "\n",
    "        if use_clipped_model_output:\n",
    "            # the pred_epsilon is always re-derived from the clipped x_0 in Glide\n",
    "            pred_epsilon = (sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)\n",
    "\n",
    "        # 6. compute \"direction pointing to x_t\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * pred_epsilon\n",
    "\n",
    "        # 7. compute x_t without \"random noise\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
    "        prev_sample_mean = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n",
    "\n",
    "        if eta > 0 and prev_sample is None:\n",
    "            if variance_noise is not None and generator is not None:\n",
    "                raise ValueError(\n",
    "                    \"Cannot pass both generator and variance_noise. Please make sure that either `generator` or\"\n",
    "                    \" `variance_noise` stays `None`.\"\n",
    "                )\n",
    "\n",
    "            if variance_noise is None:\n",
    "                variance_noise = randn_tensor(\n",
    "                    model_output.shape, generator=generator, device=model_output.device, dtype=model_output.dtype\n",
    "                )\n",
    "            variance = std_dev_t * variance_noise\n",
    "\n",
    "            prev_sample = prev_sample_mean + variance\n",
    "\n",
    "        if not return_dict:\n",
    "            return (prev_sample,)\n",
    "        \n",
    "        log_prob = self._get_logprob(prev_sample, prev_sample_mean, std_dev_t)\n",
    "\n",
    "        return prev_sample, log_prob\n",
    "    \n",
    "    def _get_logprob(self, x, mean, std):\n",
    "        # x_clone = x.clone()\n",
    "        # mean_clone = mean.clone()\n",
    "        log_prob = -((x-mean)**2 / 2 * (std**2 + 1e-5)) - (std + 1e-5).log() - tensor(torch.pi * 2).log()\n",
    "        return log_prob.mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb3857-61f9-4a71-9e32-bf20695ae5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='vae').to('cuda')\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule='scaled_linear')\n",
    "sd_model = UNet2DConditionModel.from_pretrained('runwayml/stable-diffusion-v1-5', subfolder='unet').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fbefb-7d5c-47c9-90d0-c57d19515285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b11d00-c8ee-4e12-a258-4c7c420ec763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRACB(Callback):\n",
    "    order = DeviceCB.order + 1\n",
    "    def before_fit(self):\n",
    "        lora_attn_procs = {}\n",
    "        for name in self.learn.model.attn_processors.keys():\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else self.learn.model.config.cross_attention_dim\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = self.learn.model.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(self.learn.model.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = self.learn.model.config.block_out_channels[block_id]\n",
    "                \n",
    "            lora_attn_procs[name] = LoRAAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "        \n",
    "        self.learn.model.set_attn_processor(lora_attn_procs)\n",
    "        self.learn.lora_layers = AttnProcsLayers(self.learn.model.attn_processors).to(self.learn.model.device)\n",
    "        self.learn.opt = self.learn.opt_func(self.learn.lora_layers.parameters(), self.learn.lr)\n",
    "        # for pg in self.learn.opt.param_groups: pg['params'] = self.learn.lora_layers.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6a9bc-49ee-45bd-9d3a-4199eb38f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570c881-13b2-4819-8cff-03f9c6ef6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointCB(Callback):\n",
    "    def __init__(self, model_name, file_path, lora=False, safetensors=True, checkpoint_intervals=None): \n",
    "        super().__init__()\n",
    "        fc.store_attr()\n",
    "        if not file_path.endswith(\"/\"): self.file_path = f\"{self.file_path}/\"\n",
    "        if not Path(file_path).exists(): os.makedirs(file_path)\n",
    "        self.checkpoint_counter = 0\n",
    "    def after_batch(self):\n",
    "        if self.checkpoint_intervals is not None and self.learn.batch_idx % self.checkpoint_intervals == 0:\n",
    "            weights = self.learn.model.state_dict()\n",
    "            opt = self.learn.opt.state_dict()\n",
    "            folder = os.makedirs(f'{self.file_path}{self.model_name}_checkpoint_{self.checkpoint_counter}/', exist_ok=True)\n",
    "            if self.safetensors:\n",
    "                save_file(weights, f\"{folder}model_weights\")\n",
    "                save_file(weights, f\"{folder}optimiser\")\n",
    "            else:    \n",
    "                torch.save(weights, f\"{folder}model_weights.bin\")\n",
    "                torch.save(opt, f\"{folder}optimiser.bin\")\n",
    "            self.checkpoint_counter += 1\n",
    "    def after_fit(self):\n",
    "        save_params = self.learn.lora_layers.state_dict() if self.lora else self.learn.model.parameters()\n",
    "        if self.safetensors: save_file(save_params, f\"{self.file_path}{self.model_name}_lora\")\n",
    "        else: torch.save(save_params, f\"{self.file_path}{self.model_name}_lora.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295b959-140e-4c8d-8e28-59892d1d0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3a761-982a-44f2-9056-fc3d8469d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPOCB(MixedPrecisionCB):\n",
    "    order = LoRACB.order + 1\n",
    "    def __init__(\n",
    "        self, \n",
    "        reward_func, \n",
    "        scheduler,\n",
    "        batch_size=256, \n",
    "        denoising_steps=50, \n",
    "        train_steps=50, \n",
    "        clip_max=10.0, \n",
    "        clip_range=1e-4, \n",
    "        guidance_scale=5., \n",
    "        mixed_precision=torch.float16,\n",
    "        n_inp=1\n",
    "    ): \n",
    "        super().__init__(mixed_precision, n_inp)\n",
    "        fc.store_attr()\n",
    "        self.running_stats = RunningStats()\n",
    "        self.gradient_accumulation_steps = self.batch_size // 4\n",
    "    def before_fit(self):\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        self.prev_model = copy.deepcopy(self.learn.model).requires_grad_(False).to('cuda')\n",
    "        self.learn.rewards = []\n",
    "        self.learn.norm_rewards = []\n",
    "        self.learn.losses = []\n",
    "    def predict(self):\n",
    "        self.batch_losses = []\n",
    "        prompts, seeds = self.learn.xb\n",
    "        device = prompts.device\n",
    "        \n",
    "        # sample\n",
    "        sample_dict = {\n",
    "            s_id: {\n",
    "                \"prompt\": prompts[s_id],\n",
    "                \"final_image\": 0,\n",
    "                \"reward_norm\": 0,\n",
    "                \"latents\": [],\n",
    "                \"next_latents\": [],\n",
    "                \"log_prob\": [],\n",
    "                \"seed\": seeds[s_id]\n",
    "            } \n",
    "            for s_id in range(len(prompts))\n",
    "        }\n",
    "        self.samples = sample(sample_dict, self.prev_model, self.scheduler)\n",
    "        rewards = torch.cat([self.reward_func(id2str[s['prompt'].item()], s['final_image']) \n",
    "                        for s in progress_bar(self.samples.values(), leave=False, comment='calculating rewards...')])\n",
    "        norm_rewards = self.running_stats.get_norm_rewards(prompts, rewards).to(device)\n",
    "        \n",
    "        # logging\n",
    "        for i in range(len(norm_rewards)): \n",
    "            self.learn.rewards.append(rewards[i].cpu().item())\n",
    "            self.learn.norm_rewards.append(norm_rewards[i].cpu().item())\n",
    "        \n",
    "        # training loop\n",
    "        t_idx = [tensor(random.sample(range(self.denoising_steps), self.train_steps)) for i in range(len(self.samples))]\n",
    "        self.learn.model.train()\n",
    "        for step in progress_bar(range(self.train_steps), leave=False, comment='training...'):\n",
    "            self.scheduler.set_timesteps(self.train_steps)\n",
    "            batch_t = torch.stack([self.scheduler.timesteps[ts_list[step]] for ts_list in t_idx]*2)\n",
    "            batch_latents = torch.cat([self.samples[i]['latents'][ts_list[step]] for i, ts_list in enumerate(t_idx)]*2)\n",
    "            batch_embeds = torch.cat([\n",
    "                torch.cat([id2embed[135] for i in range(len(self.samples))]), # unconditional embeddings\n",
    "                torch.cat([id2embed[s['prompt'].item()] for s in self.samples.values()]) # conditional embeddings\n",
    "            ]).to(device)\n",
    "\n",
    "            uncond, cond = self.learn.model(batch_latents, batch_t.to(device), batch_embeds).sample.chunk(2)\n",
    "            noise_pred = uncond + self.guidance_scale * (cond - uncond)\n",
    "            batch_next_latents = torch.cat([self.samples[i]['next_latents'][ts_list[step]] for i, ts_list in enumerate(t_idx)])\n",
    "            batch_logprob = torch.stack([self.samples[i]['log_prob'][ts_list[step]] for i, ts_list in enumerate(t_idx)])\n",
    "            \n",
    "            log_probs = []\n",
    "            for i in range(len(batch_next_latents)):\n",
    "                n, t, l, nl = noise_pred[i][None,:], batch_t[i], batch_latents[i][None,:], batch_next_latents[i][None,:]\n",
    "                _, log_prob = self.scheduler.step(n, t, l, prev_sample=nl)\n",
    "                if log_prob == torch.nan: print(step)\n",
    "                log_probs.append(log_prob)\n",
    "\n",
    "            # ddpo_is objective\n",
    "            ip_ratio = (torch.stack(log_probs) - batch_logprob).exp()\n",
    "            norm_rewards = norm_rewards.clip(-self.clip_max, self.clip_max)\n",
    "            loss_unclipped = -ip_ratio * norm_rewards\n",
    "            loss_clipped = -ip_ratio.clip(1. - self.clip_range, 1. + self.clip_range) * norm_rewards\n",
    "            self.learn.loss = torch.stack([max(z) for z in zip(loss_unclipped, loss_clipped)]).mean()\n",
    "            self.batch_losses.append(self.learn.loss.detach().cpu().item())\n",
    "            self.scaler.scale(self.learn.loss).backward()\n",
    "        \n",
    "        self.learn.losses.append(tensor(self.batch_losses).mean().cpu().item())\n",
    "    \n",
    "    def get_loss(self): return\n",
    "    def backward(self): return\n",
    "    def step(self):\n",
    "        if (self.learn.batch_idx + 1) % self.gradient_accumulation_steps == 0: \n",
    "            self.scaler.unscale_(self.learn.opt)\n",
    "            clip_grad_norm_(self.learn.lora_layers.parameters(), 1.0)\n",
    "            self.scaler.step(self.learn.opt)\n",
    "            self.scaler.update()\n",
    "    def zero_grad(self):\n",
    "        if (self.learn.batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "            self.learn.opt.zero_grad()\n",
    "        if (self.learn.batch_idx + 1) % self.batch_size == 0:\n",
    "            self._save_model()\n",
    "    def _save_model(self):\n",
    "        self.prev_model.load_state_dict(self.learn.model.state_dict())\n",
    "        self.prev_model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbaa157-9155-4a34-9a6c-9db7e34ad66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPOProgressCB(ProgressCB):\n",
    "    def __init__(self, plot=False): \n",
    "        self.plot = plot\n",
    "        if plot: self.losses, self.counter = [], 0\n",
    "    def after_batch(self):\n",
    "        mbar = self.mbar\n",
    "        if not hasattr(mbar, 'graph_fig'):\n",
    "            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 3, figsize=(18, 3.5))\n",
    "            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n",
    "            \n",
    "        titles = ['Reward', 'Normalised Reward', 'Loss']\n",
    "        items = [self.learn.rewards, self.learn.norm_rewards, self.learn.losses]\n",
    "        for i, (title, item) in enumerate(zip(titles, items)):\n",
    "            mbar.graph_axs[i].clear()\n",
    "            mbar.graph_axs[i].plot(item, '.', alpha=0.3)\n",
    "            mbar.graph_axs[i].set_ylim(tensor(item).min(), tensor(item).max())\n",
    "            mbar.graph_axs[i].set_title(title)\n",
    "        \n",
    "        # Update graph\n",
    "        mbar.graph_out.update(mbar.graph_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a04bf-0312-4349-adf0-6c9fbd0fac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epochs = 1e-5, 5\n",
    "llava_prompt = \"what is happening in this image?\"\n",
    "reward = partial(prompt_alignment_score, query=llava_prompt, llava_model=llava_model, clip_model=image_processor, llava_tokenizer=llava_tokenizer, bert=bert, return_all=False)\n",
    "ddpo = DDPOCB(reward, scheduler)\n",
    "path = '/home/projects/reinforcement_finetuning/models/'\n",
    "checkpoint = CheckpointCB(\"ddpo_1\", path, checkpoint_intervals=5000)\n",
    "cbs = [DeviceCB(), MetricsCB(), LoRACB(), checkpoint, ddpo, DDPOProgressCB()]\n",
    "learn = Learner(dls, sd_model, cbs=cbs, opt_func=partial(optim.AdamW, weight_decay=1e-4, eps=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4aad2e-643b-47fd-a652-7b8eb0f7dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
